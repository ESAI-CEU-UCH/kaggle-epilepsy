\documentclass[a4paper,english,twoside]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{babel}
\usepackage{xspace}
\usepackage{amsmath}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{calc}
\usepackage{anysize}
\usepackage{url}
\usepackage{hyperref}
\usepackage{libertine}
\usepackage[libertine]{newtxmath}

\marginsize{3cm}{3cm}{3cm}{3cm}

%\renewcommand{\familydefault}{\sfdefault}

%% %% Cabecera - Start %%
\fancyhf{}
\fancyhead[LO]{\small Seizure prediction system of ESAI-CEU-UCH team}
\fancyhead[RE]{\small F. Zamora-Martínez, F.J. Muñoz-Almaraz, P. Botella-Rocamora, J. Pardo}
\fancyhead[RO,LE]{}
\fancyfoot[C]{\bfseries \thepage}
\renewcommand{\headrulewidth}{0.5pt}  % headrule
\pagestyle{fancy}
%% %% Cabecera - End %%

\newcommand{\etc}{etc.\@\xspace}
% \setlength{\extrarowheight}{-5em}

\newcommand{\argmax}{\mathop{\mbox{argmax}}}

% \name{}
\author{Francisco Zamora-Martínez, Francisco J. Muñoz-Almaraz,\\
  Paloma Botella-Rocamora, Juan Pardo}
\date{November 2014}
% \address{}
\title{\ Seizure prediction using FFT, eigen values of correlation matrix,
  artificial neural networks, k-nearest-neighbors and Bayesian model combination}

\begin{document}
\maketitle

\begin{itemize}
\item[Location:] Universidad CEU Cardenal Herrera, Alfara del Patriarca,
  Valencia, Spain. Embedded Systems and Artificial Intelligence (ESAI) research
  group.
\item[Email:] \{francisco.zamora, malmaraz, pbotella, juaparal\}@uch.ceu.es
\item[Competition:] American Epilepsy Society Seizure Prediction Challenge.
\end{itemize}

\section{Summary}\label{summary}

This report presents the system proposed by Universidad CEU Cardenal Herrera
(ESAI-CEU-UCH team) at Kaggle American Epilepsy Society Seizure Prediction
Challenge. The proposed system was positioned as \textbf{4th} at Kaggle
leaderboard~\cite{kaggle}. Different kind of input features (different
preprocessing pipelines) and different statistical models are being
proposed. This diversity was motivated to allow model combination to improve the
result.

It is important to note that none of the proposed systems use test set
for any calibration or related purpose. The competition rules allow to do this
model calibration using test set, but doing it will reduce the
reproducibility of the results in a real world implementation.

\section{Feature selection and
  extraction}\label{feature-selection-and-extraction}

Several preprocessing pipelines have been performed. The most relevant
preprocessing technique according to final performance is FFT plus PCA
procedure. This pipeline combines methods implemented in R~\cite{Rproject} and
others in APRIL-ANN toolkit~\cite{aprilann}.

\subsection{FFT features plus PCA/ICA}\label{fft-features-plus-pcaica}

This is the most important kind of proposed features, obtaining the best
standalone result. This process follows the following steps for each input
file~\cite{2014:howbert:plosone}:

\begin{enumerate}
\item
  Extract windows of 60 seconds size with 50\% overlapping for every
  channel in the data, increasing the number of samples in the data set.
  We obtained 19 windows for every input file. Every sample is filtered
  by using Hamming windows.
\item
  For every window, real FFT is computed using an algorithm which needs power of
  two window sizes. The 60s input window has 24\,000 samples for Dogs and
  300\,000 samples for Patients, leading into 16\,384 FFT bins for Dogs and
  262\,144 FFT for Patients.
\item A filter bank is computed for bands: delta (0.1Hz --- 4Hz), theta (4Hz ---
  8Hz), alpha (8Hz --- 12Hz), beta (12Hz --- 30Hz), low-gamma (30Hz --- 70Hz),
  high-gamma (70Hz --- 180Hz). The mean in the corresponding frequency bands has
  been computed.
\item
  The output of the filter bank is compressed by computing the logarithm
  of the values.
\end{enumerate}

For every file, this FFT preprocess computes a matrix with 19 rows (windows of
60s with 50\% overlapping) and $6C$ columns being $C$ the number of
channels. This transformation has been applied to all the available data
(training and test).

A PCA transformation has been computed over the concatenation by rows of all
training data matrices (ignoring test data). Data have been centered and scaled
before PCA. After computing centers, scales and PCA matrix transformation, them
have been applied to all the available data (training and test). In a similar
fashion to PCA, Independent Component Analysis (ICA) has been performed. The
advantage of PCA/ICA transformation is their ability to remove linear
correlations between features.\footnote{ICA transformation needs a random number
  generator and the seed was random during competition, so it is not possible to
  reproduce exactly the competition result.}

\subsection{Eigen values of pairwise channels correlation
  matrix}\label{eigen-values-of-pairwise-channels-correlation-matrix}

This preprocessing was based in previous competition system of Michael
Hills~\cite{michaelhills}.

\subsubsection{Windowed correlation
  matrix}\label{windowed-correlation-matrix}

Correlations between the channels with windows covering 60\,s are calculated and
the eigenvalues of the correlation matrix are taken as features for training
with KNN and ANN models (see Section~\ref{modelling-techniques-and-training}).
Every 10 minutes segment provided in the competition data set generates 19
signal subintervals whose lapse is 60\,s overlapping 30\,s with the previous and
the posterior subintervals.  Each subinterval corresponds to a data set that
consists in a matrix whose size is the number of channels times the product of
the sampling frequency and 60.  For every subinterval (window covering 60\,s)
the eigenvalues of its correlation matrix are calculated and stored in a new
file. With the former procedure, the number of covariables got is 19 times the
number of channel in the individual.  Therefore, the number of features is
different depending on the individual. This features jointly with FFT (with and
without PCA/ICA transformation) were used as inputs, and it will be denoted as:
FFT+CORW, PCA+CORW, ICA+CORW.\footnote{PCA/ICA transformation is only applied to
  FFT features, not to CORW features.}

Correlations were also calculated with the FFT transformation of every
sequence described in section~\ref{fft-features-plus-pcaica},
but the results were not stable enough.

\subsubsection{Global correlation
  matrix}\label{global-correlation-matrix}

A whole segment file provided in the competition is analysed to find
correlations between channels.  The set of features got with this analysis was
used with KNN models for training.  Not only is considered the original signal
in the sequence, but also a differentiated signal consisting in the difference
of two consecutive values of the original signal. The original signal is a
matrix whose size is the number of channels times the product of the frequency
sampling and 600. The functions \verb@eigen@ and \verb@cor@ in R are applied to
this matrix and the output are the eigenvalues of the correlation matrix. The
number of eigenvalues is equal to the number of channels for each individuals.

For each channel signal, a transformed signal is got with the difference between
two consecutive values of the signal.  The correlations of these differentiated
signals form new features that added to the correlation of the original signals
increase the predictive performance with K-Nearest-Neighbor (KNN) models. The
differentiated signals of a segment is a matrix whose size is the number of
channels times the product of the frequency sampling and 599. Applying
\verb@eigen@ and \verb@cor@ functions, the eigenvalues of the correlation matrix
of these differentiated signals are calculated.

With these procedures, the number of features got is the double of the number of
channels and they are stored in a new file. This feature will be denoted as CORG
in the following.

The main flaw observed is that for human patients the differentiated signal
does not provide information due to patient data have a higher sampling frequency. We
suppose that the difference of two values in a human signal separated by a lapse
of time similar to the dog signals (1/400\,s) could improve the quality of these
features.  Nevertheless, being a minor feature, in the final track of the
competition we pay more attention to the most prominent preprocessing.

\subsection{Global statistical
  features}\label{global-statistical-features}

Exploratory analysis of original data showed different behavior in the signal
variability of both classes, pre-ictal and inter-ictal. Standard deviation were
calculated for every data set in every channel.  Following the study of
discriminant statistical features, a fourier basis representation (with 15
elements) was calculated for every data set. Exploratory analysis on their
coefficients showed different behavior between pre-ictal and inter-ictal
series. Pre-ictal signals coefficients displayed lower coefficients (closer to
0) while inter-ictal signals coefficients presented greater disparity between
them. As a summary measure of this behavior, standard deviation of these
coefficients was calculated for each data serie, once per channel, and the mean
standard deviation was calculated for all channels to reduce the dimensionality
and avoid redundant information. This feature will be denoted as COVRED in the
following.

\section{Modelling techniques and
  training}\label{modelling-techniques-and-training}

\subsection{Cross-validation
  algorithm}\label{cross-validation-algorithm}

A cross-validation approach has been followed to compute AUC estimate using
training data. All models have been trained independently, but using the same
hyper-parameters and configurations.

The number of folds per subject is the maximum number of full
sequences\footnote{A full sequence are those where all sequence numbers are
  available, not all the available sequences are full.} in pre-ictal data. Folds
are build in such way that all segments in the same sequence belong to the same
fold. Both, pre-ictal and inter-ictal data have been splitted into the same
number of folds, and following the same sequence constraint.  For KNNs,
cross-validation allows to compute an estimate of the AUC, but the final model
contains all the available training data.  For Artificial Neural Networks
(ANNs), one model has been trained for each possible partition, and the test
result is the mean of every model output.

\subsection{Models}\label{models}

Different models have been tested in APRIl-ANN toolkit. Logistic regression as
initial baseline following a simple linear regression approach, which was not
introduced in the final system due to convergence problems.  In order to exploit
non-linear relations in data, KNNs and dropout ANNs with different number of
layers and ReLU as neurons have been trained. And finally, an ensemble of
several models has been performed.

\subsubsection{General aspects}

Two different model specifications are possible, depending on the kind of input
features: global features model and windowed features model. The case of global
features model is the most simple, correspond to models whose inputs are CORG and
COVRED features. The model receives as input all the features extracted from
one segment file, and computes how likely this file is a pre-ictal sample. In
case of windowed features model, the model receives a file segment splitted
following a sliding window over temporal axis, and computes how likely every
window is a precital sample. Once all windows probabilities have been computed,
pre-ictal probability of the file segment is computed as the complementary of the
geometric mean of inter-ictal probability:

\begin{equation}
p(c=\text{pre-ictal} | x) = 1 - \sqrt[n]{\prod_{t=1}^n (1 - p_t)}
\label{eq:gmean}
\end{equation}

\noindent where $x$ is a file segment, $n$ is the number of time windows in $x$
and $p_t$ is the pre-ictal probability of window $t$. Different aggregation
methods, as arithmetic mean, maximum or harmonic mean, were tested achieving
similar or worst results.

\subsubsection{K-Nearest Neighbors}\label{k-nearest-neighbors}

KNN models were chosen after convergence problems with logistic regression
models. KNNs are non-parametric and don't need to be trained because training
data is the model. The number of neighbors hyper-parameter has been set to
$K=40$ after trying different values and looking into cross-validation and
public AUC scores. The KNN hypotheses were transformed into probabilities
following the implementation given in APRIL-ANN toolkit, which is based
on~\cite{2005:nips:hinton:NCA}:

\begin{equation}
p(c=\text{pre-ictal} | s) =
\displaystyle{\frac{\displaystyle{\sum_{ s^\prime \in \text{pre-ictal}(\mathcal{K}) } exp( -||s - s^\prime||^2_2 ) }}
{\displaystyle{\sum_{s^\prime \in \mathcal{K}} exp( -||s - s^\prime||^2_2 ) }}}
\end{equation}

\noindent where $s$ is an input sample\footnote{It can be a global feature
  vector computed over the whole segment file, or a temporal window taken from
  the file.}, $\mathcal{K}$ is the set of hypoteses given by the KNN,
$\text{pre-ictal}(\mathcal{K})$ is a subset of $\mathcal{K}$ containing only
samples belonging to pre-ictal class. The value of $K$ and the computation of
probabilities using distances allow to reduce the impact of overfitting in the
KNN model.

\subsubsection{Artificial Neural
  Networks}\label{artificial-neural-networks}

ANN models with different hyper-parameters were tested, but in a non exhaustive
way because of the large gap between cross-validation AUC and public test
AUC. Overfitting effect was reduced by introducing a 50\%
dropout~\cite{2012:arxiv:hinton:dropout} in all hidden layers and a Gaussian
additive noise with mean $0$ and variance $0.04$ in the inputs. All proposed
ANNs have ReLU~\cite{2011:glorot:aistats} as activation functions in all hidden
layers, and logistic activation function at the output layer. Training was
performed by stochastic gradient descent with mini-batches (bunch size) of 128
samples and minimizing the cross-entropy loss function. Validation loss was also
cross-entropy but after aggregating all the probabilities computed for each
segment file following the Equation~(\ref{eq:gmean}). The input of the ANN was
extended taking into account three windows (the current, the previous one and
the following), allowing to capture temporal patterns in data. The learning rate
was set to $\eta_0=0.2$ with a decay parameter of $\epsilon=0.001$, momentum was
set to $\gamma=0.1$ and no other regularization term has been used. ANN weights
were initialized sampling from a uniform distribution in range $[-3,3]$. One
weight $w$ at iteration $t+1$ is computed following:

\begin{eqnarray}
\eta &=& \frac{\eta_0}{1 + \epsilon t}\\
w^{(t+1)} &=& w^{(t)} - \eta \frac{\partial L}{\partial w^{(t)}} + \gamma ( w^{(t)} - w^{(t-1)} )
\end{eqnarray}

\noindent being $L$ the cross-entropy loss of the mini-batch.

Different hyper-parameters, including L1 and L2 regularization were tested without
good results. Again, it is important to take into account that all these
hyper-parameters were selected in a non-exhaustive way.

\subsubsection{Ensemble of models}\label{ensemble-of-models}

All the proposed models have been trained independently, computing
cross-validation probabilities for every training file, and test output
submission file, and the final system is a combination of these outputs.
Initially, a simple uniform linear combination of probability outputs in
submission files has been tested. In order to improve this result, a Bayesian
Model Combination (BMC) algorithm~\cite{2011:monteith:ijcnn} has been
implemented in Lua for APRIL-ANN~\cite{aprilann} toolkit. BMC algorithm uses
cross-validation output probabilities to optimize the likelihood of the
combination, and uses these weights to combine output probabilities in
submission files.

\subsection{Results}\label{results}

The best submitted model is the BMC ensemble of the described models
above. They achieved a \textbf{0.7935} AUC in private test.

\section{Dependencies and solution recipe}\label{dependencies}

This system uses the following open source software:

\begin{itemize}
\item APRIL-ANN toolkit v0.4.0~\cite{aprilann}. It is a toolkit for pattern
  recognition with Lua scripting and a C/C++ core.  Because this tool is very
  new, the installation and configuration has been written in the pipeline.
\item R project v3.0.2~\cite{Rproject}. For statistical
  computing, a wide spread tool in Kaggle competitions. Packages
  R.matlab, MASS, fda.usc, fastICA, stringr and plyr are necessary to
  run the system.
\item GNU BASH v4.3.11~\cite{bash} with standard command line tools.
\end{itemize}

The system is prepared to run in a Linux system with Ubuntu 14.04 LTS, but it
can be run in other Debian based distributions, but not tested.

See the README file at GitHub
repository\footnote{\url{https://github.com/ESAI-CEU-UCH/kaggle-epilepsy}} for
deeper information and the solution recipe howto,

\section{Code description}\label{code-description}

The code is a bunch of different scripts for APRIL-ANN and R, plus bash-scripts
for the recipe automation. The folder \verb+scripts/+ contains the different
programs, which have been organized depending in their purpose.

The following scripts are for configuration and common functions:

\begin{itemize}
\item \verb+scripts/configure.sh+: is a bash-script which downloads APRIL-ANN,
  install dependencies, and compiles it using ATLAS library by default.
\item \verb+scripts/configure.R+: is an R script which install the needed
  R packages at the user home folder.
\item \verb+scripts/cmd.lua+: contains Lua code for APRIL-ANN with command
  line options of the training scripts.
\item \verb+scripts/common.lua+: contains Lua code for APRIL-ANN with several
  functions which were shared between different preprocessing and training
  programs.
\end{itemize}

The following ones are for preprocessing:

\begin{itemize}
\item \verb+scripts/PREPROCESS/compute_fft.lua+: is a Lua script for APRIL-ANN
  which computes the FFT preprocess, before PCA/ICA transformation, indicated at
  Section~\ref{fft-features-plus-pcaica}.
\item \verb+scripts/PREPROCESS/compute_pca.R+: is an R script which computes
  PCA transformation matrices: rotation, center and scale matrices.
\item \verb+scripts/PREPROCESS/compute_ica.R+: is an R script which computes ICA
  transformation matrices: rotation, center and scale matrices.
\item \verb+scripts/PREPROCESS/apply_pca.lua+: is a Lua script for APRIL-ANN
  which takes the PCA transformation matrices and computes PCA rotation of all
  data.
\item \verb+scripts/PREPROCESS/apply_ica.lua+: is a Lua script for APRIL-ANN
  which takes the ICA transformation matrices and computes ICA of all data.
\item \verb+scripts/PREPROCESS/correlation_60s_30s.R+: is a R script which
  computes windowed correlation matrix as described at
  Section~\ref{windowed-correlation-matrix}.
\item \verb+scripts/PREPROCESS/compute_corg.R+: is a R script which computes
  global correlation matrix as described at
  Section~\ref{global-correlation-matrix}.
\item \verb+scripts/PREPROCESS/compute_covarred.R+: is a R script which computes
  global statistical features as described at
  Section~\ref{global-statistical-features}.
\end{itemize}

\noindent Note that this preprocessing scripts produces intermediary output
results in disk storage.

The following are for training+testing:

\begin{itemize}
\item \verb+scripts/MODELS/train_one_subject_knn.lua+: is a Lua script for
  APRIL-ANN which computes cross-validation results for KNN model and produces
  the test outputs. This script has a lot of configuration options, the most
  promising where selected during competition and frozen in the solution at
  \verb+scripts/MODELS/confs/knn*.lua+.
\item \verb+scripts/MODELS/train_one_subject_mlp.lua+: is a Lua script for
  APRIL-ANN which trains a MLP following the cross-validation scheme and
  produces its outputs. This script has a lot of configuration options, the most
  promising where selected during competition and frozen in the solution at
  \verb+scripts/MODELS/confs/ann*.lua+.
\item \verb+scripts/MODELS/train_all_subjects_wrapper.lua+: is a Lua script for
  APRIL-ANN which takes one of the two above scripts and trains the model for
  all the available subjects, producing the submission for Kaggle.
\end{itemize}

The following are for models ensemble:

\begin{itemize}
\item \verb+scripts/ENSEMBLE/naive_ensemble.lua+: is a Lua script for
  APRIL-ANN which implements an uniform linear combination of several test
  outputs.
\item \verb+scripts/ENSEMBLE/bmc_ensemble.lua+: is a Lua script for APRIL-ANN
  which implements BMC ensemble for models trained following the
  cross-validation scheme, and produces the corresponding test output.
\end{itemize}

The following are only for testing:

\begin{itemize}
\item \verb+scripts/MODELS/test_one_subject_knn.lua+: is a Lua script for
  APRIL-ANN which computes test results for a KNN model and one subject.
\item \verb+scripts/MODELS/test_one_subject_mlp.lua+: is a Lua script for
  APRIL-ANN which computes test results for a MLP model and one subject.
\end{itemize}

\section{Additional comments and
  observations}\label{additional-comments-and-observations}

Among the indicated as submitted best system, different models and
features combinations have been tested. Table~\ref{tab:val} summarizes
the cross-validation AUC and public test AUC for the most important
combinations. In Table~\ref{tab:val} first column contains the model, being one
of the following:

\begin{itemize}
\item
  LR: Logistic regression.
\item
  KNN: K-Nearest-Neighbors with $K=40$.
\item ANN: Artificial Neural Networks with 50\% dropout and
  ReLU~\cite{2011:glorot:aistats} neurons, 128 units in every hidden layer, and
  a logistic activation function at the output.  ANN2 for two hidden layers,
  ANN3 for three hidden layers, and so on. ANN2p for two hidden layers with 64
  units in every layer,
\item
  UNIFORM or BMC: ensemble of the most promising systems, indicated in
  bold face. UNIFORM is a linear combination with uniformly distributed
  weights; BMC is linear combination where weights are estimated
  following BMC optimizing cross-validation likelihood.
\end{itemize}

The second column indicates which features were used as input of the
model, being one of the following list:

\begin{itemize}
\item
  FFT: the output of the proposed filter bank plus logarithm
  compression.
\item
  FFT+CORW: the previous one plus windowed eigen values of correlation
  matrix.
\item
  PCA+CORW: the PCA transformation of FFT features plus windowed eigeb
  values of correlation matrix.
\item
  ICA+CORW: idem as previos one, but using ICA instead of PCA.
\item
  CORG: correlation using 10 minutes window.
\item
  COVRED: different statistical measures over the original signal.
\item
  ENSEMBLE: output probabilities of the bold faced systems.
\end{itemize}

\begin{table}
  \centering
  \begin{tabular}{|l|l|cc|}
    \hline
    Model & Features & CV AUC & Pub. AUC\\
    \hline
    \hline
    LR & FFT & 0.9337 & 0.6784\\
    \hline
    KNN & FFT & 0.8008 & 0.6759\\
    KNN & FFT+CORW & 0.7994 & 0.7040\\
    \textbf{KNN} & \textbf{PCA+CORW} & \textbf{0.8104} & \textbf{0.7288}\\
    \textbf{KNN} & \textbf{ICA+CORW} & \textbf{0.8103} & \textbf{0.6840}\\
    \hline
    ANN2 & FFT+CORW & 0.9072 & 0.7489\\
    ANN2 & PCA+CORW & 0.9082 & 0.7815\\
    \textbf{ANN2p} & \textbf{PCA+CORW} & \textbf{0.9175} & \textbf{0.7895}\\
    \textbf{ANN2} & \textbf{ICA+CORW} & \textbf{0.9104} & \textbf{0.7772}\\
    ANN3 & PCA+CORW & 0.9188 & 0.7690\\
    ANN4 & PCA+CORW & 0.9268 & 0.7772\\
    \textbf{ANN5} & \textbf{PCA+CORW} & \textbf{0.9283} & \textbf{0.7937}\\
    ANN6 & PCA+CORW & 0.9291 & 0.7722\\
    \hline
    \textbf{KNN} & \textbf{CORG} & \textbf{0.7097} & \textbf{0.6552}\\
    \textbf{KNN} & \textbf{COVRED} & \textbf{0.6900} & \textbf{0.6901}\\
    \hline
    UNIFORM & ENSEMBLE & --- & 0.8048\\
    BMC & ENSEMBLE & 0.9271 & 0.8249\\
    \hline
  \end{tabular}
  \caption{Cross-validation and public AUC results for the most important
    systems tested during the competition.\label{tab:val}}
\end{table}            

Table~\ref{tab:val} shows that logistic regression model was the worst in
terms of generalization ability. The application of PCA allows to
improve the public AUC results by \textbf{0.025} for KNN model and
\textbf{0.033} for ANN model. ICA obtains improvements similar to PCA.
The advantage of deep ANNs (with more than two hidden layers) is not
clear, but the best public AUC was obtained by an ANN with five hidden
layers, improving by \textbf{0.004} the public AUC of the best ANN2.
Global features as CORG and COVRED show strong correlation between
cross-validation AUC and public test AUC. Finally, the use of BMC
ensemble method allow to improve the public AUC in \textbf{0.031} points
compared to the ANN5 model.

Finally, the private AUC for the two selected models is shown at
Table~\ref{tab:private}. Note that this table shows private AUC for competition
system, and private AUC for post-competition system using MKL or ATLAS
library. As it is indicated in the README at GitHub repository, the competition
system couldn't be reproduce in an exact way due to a non frozen seed number.
Now, the code has this number frozen, and its AUC is shown in the table. In
addition to this problem, our system uses Intel MKL library for the best performance
in experimentation. However, the code is configured by default to compile using
ATLAS library, which is open source and standard in Linux systems. The
post-competition private AUC for MKL and ATLAS options are also shown in the
table for comparison purposes.\footnote{Note that the system hyper-parameters
  have been optimized using MKL compilation flag, so it is expected worst
  results compiling with ATLAS library.}

\begin{table}
  \centering
  \begin{tabular}{|l|l|c||c|c|}
    \hline
          &          & Priv. AUC & \multicolumn{2}{|c|}{Post. priv. AUC}\\
    Model & FEATURES & MKL & \multicolumn{1}{|c}{MKL} & \multicolumn{1}{c|}{ATLAS} \\
    \hline
    \hline
    ANN5 & PCA+CORW & 0.7644          & 0.7644 & 0.7405\\
    BMC & ENSEMBLE & \textbf{0.7935}  & 0.7983 & 0.7910\\
    \hline
  \end{tabular}
  \caption{Private AUC for the two selected submissions. Post. priv. AUC refers
    to AUC results in private test after the competition.\label{tab:private}}
\end{table}

The private AUC of the BMC ensemble is the second best if we compare it
to all our submissions, however, the difference with the best private
AUC is not significant, the best one has a private AUC of
\textbf{0.7958}.

The experimentation has been performed in a cluster of \textbf{three}
computers with same hardware configuration:

\begin{itemize}
\item
  Dell PowerEdge 210 II server rack.
\item
  Intel Xeon E3-1220 v2 at 3.10GHz with 16GB RAM (4 cores).
\item
  2.6TB of NFS storage.
\end{itemize}

\section{Simple features and methods}\label{simple-features-and-methods}

The best single features plus model was ANN5, an artificial neural
network with 5 hidden layers using ReLUs~\cite{2011:glorot:aistats}
as activation function, with 128 neurons in every layer and logistic
activation function as output. The input features were FFT+CORW as
described above. This system achieves a \textbf{0.7644} AUC in the
private test.

\bibliography{refs}
\bibliographystyle{plain}

\end{document}
